import copy
import pickle

from confluent_kafka import Consumer, Producer

from ..run_engine import Dispatcher, DocumentNames


def delivery_report(err, msg):
    """ Called once for each message produced to indicate delivery result.
        Triggered by poll() or flush(). """
    if err is not None:
        print('Message delivery failed: {}'.format(err))
    else:
        print('Message delivered to {} [{}]'.format(msg.topic(),
                                                    msg.partition()))


class Publisher:
    """
    A callback that publishes documents to a Kafka server.

    Parameters
    ----------
    address : string
        Address of a running Kafka server as a string like
        ``'127.0.0.1:9092'``
    serializer: function, optional
        optional function to serialize data. Default is pickle.dumps.

    Example
    -------

    Publish from a RunEngine to a Kafka server on localhost on port 9092.

    >>> publisher = Publisher('localhost:9092')
    >>> RE = RunEngine({})
    >>> RE.subscribe(publisher)
    """
    def __init__(self, address, *, serializer=pickle.dumps):
        self.address = address
        self.producer = Producer(
            {
                'bootstrap.servers': self.address,
            }
        )
        self._serializer = serializer

    def __call__(self, name, doc):
        doc = copy.deepcopy(doc)
        self.producer.poll(0)
        self.producer.produce(name,
                              self._serializer(doc),
                              callback=delivery_report)
        self.producer.flush()

    def close(self):
        pass


class RemoteDispatcher(Dispatcher):
    """
    Dispatch documents received over the network from a Kafka server.

    Parameters
    ----------
    address : str or tuple
        Address of a Kafka server as a string like ``'127.0.0.1:9092'``
    deserializer: function, optional
        optional function to deserialize data. Default is pickle.loads.

    Example
    -------

    Print all documents generated by remote RunEngines.

    >>> d = RemoteDispatcher(('localhost:9092'))
    >>> d.subscribe(print)
    >>> d.start()  # runs until interrupted
    """
    def __init__(self, address, *, deserializer=pickle.loads):
        self.address = address
        self._deserializer = deserializer

        consumer_params = {
            'bootstrap.servers': self.address,
            'group.id': 'kafka-to-mongo',
            'auto.offset.reset': 'earliest'
        }
        self.consumer = Consumer(consumer_params)
        self.consumer.subscribe(topics=['start', 'descriptor', 'event', 'stop'])
        self.closed = False

        super().__init__()

    def _poll(self):
        while True:
            msg = self.consumer.poll(1.0)

            if msg is None:
                # no message was found
                pass
            elif msg.error():
                print('Consumer error: {}'.format(msg.error()))
            else:
                print('msg is "{}"'.format(msg.topic()))
                name = msg.topic()
                doc = self._deserializer(msg.value())
                print(f'"{name}":\n{doc}')
                self.process(DocumentNames[name], doc)

    def start(self):
        if self.closed:
            raise RuntimeError("This RemoteDispatcher has already been "
                               "started and interrupted. Create a fresh "
                               "instance with {}".format(repr(self)))
        try:
            self._poll()
        except:
            self.stop()
            raise

    def stop(self):
        self.consumer.close()
        self.closed = True
